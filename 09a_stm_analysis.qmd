---
title: "final_project"
author: "chris"
format: html
editor: visual
---

```{r}
if (!require("pacman")) install.packages("pacman")

pacman::p_load(
  tidyverse,
  stm,
  lubridate,
  quanteda,
  reshape2,  # For melt()
  tidytext,  # For tidy()
  ggpubr,    # For theme_pubr()
  knitr,     # For kable()
  igraph
)

# ==============================================================================
# 1. LOAD DATA & FUNCTIONS
# ==============================================================================

# Load the data we saved from the previous script
out <- readRDS("models/inputs/depression/out.rds")
dta <- readRDS("models/inputs/depression/dta.rds")

# Extract components for easy access
docs <- out$documents
vocab <- out$vocab
meta <- out$meta

# --- Define Helper Function: Comprehensive Analysis ---
analyze_topic_model <- function(topic_models_list, estimates_list) {
  
  for (i in seq_along(topic_models_list)) {
    topic_model <- topic_models_list[[i]]
    
    # Check if estimate exists for this model
    topic_key <- names(topic_models_list)[i]
    if (!topic_key %in% names(estimates_list)) {
      warning(paste("No estimate found for", topic_key, "- skipping."))
      next
    }
    estimate_model <- estimates_list[[topic_key]]
    
    topic_number <- topic_model$settings$dim$K
    print(paste("Analyzing Model with K =", topic_number))
    
    # --- NEW: Create specific directory for this model ---
    output_dir <- paste0("plots/stm/model_K", topic_number, "/")
    dir.create(output_dir, recursive = TRUE, showWarnings = FALSE)
    
    # 1. Label Topics
    top_words <- labelTopics(topic_model, topics = c(1:topic_number), n = 10, frexweight = 0.5)
    
    # 2. Topic Proportions (Gamma)
    gamma <- tidy(topic_model, matrix = "gamma", document_names = rownames(meta)) %>%
      group_by(topic) %>%
      summarise(gamma = mean(gamma)) %>%
      arrange(desc(gamma)) %>%
      mutate(topic_label = paste0("Topic ", topic),
             topic_label = reorder(topic_label, gamma))
    
    # 3. Find Thoughts (using original content from dta)
    # Note: Ensure dta$content aligns with out$meta. 
    # If out was subsetted during prep, this might need adjustment.
    thought <- findThoughts(topic_model, texts = out$meta$content, n = 10)
    
    # 4. Dominant Topic
    dominant_topic <- data.frame(dominant_topic = apply(topic_model$theta, 1, which.max))
    
    # 5. Output Text Report
    sink(paste0(output_dir, "analysis.txt"), split = TRUE)
    print("--- Summary ---")
    summary(topic_model, frexweight = 0.5)
    print("--- Gamma (Expected Proportions) ---")
    print(kable(gamma, digits = 3, col.names = c("Topic", "Mean Gamma", "Label")))
    print("--- Dominant Topic Counts ---")
    print(table(dominant_topic))
    print("--- Top Words ---")
    print(top_words)
    print("--- Top Thoughts ---")
    print(thought)
    sink()
    
    # 6. Plot: Top Word Proportions (Top 1)
    top_words_1 <- tidy(topic_model, matrix = "beta") %>%
      group_by(topic) %>%
      slice_max(beta, n = 1) %>%
      arrange(topic, -beta)
      
    p1 <- ggplot(top_words_1, aes(x = beta, y = factor(topic))) +
      geom_col(fill = "gray70") +
      geom_text(aes(label = term), hjust = 0, size = 4) +
      theme_minimal() +
      xlim(0, max(top_words_1$beta) + 0.05) +
      labs(title = paste0("Top Words (K=", topic_number, ")"), x = "Beta", y = "Topic")
    
    ggsave(paste0(output_dir, "top_word_single.png"), plot = p1, bg="white")
    
    # 7. Plot: Top Words Bar Charts (Facet)
    max_beta <- tidy(topic_model, matrix = "beta") %>%
      filter(beta > .001) %>%
      summarise(max = max(beta)) %>% pull(max)
      
    p2 <- tidy(topic_model, matrix = "beta") %>%
      group_by(topic) %>%
      slice_max(beta, n = 10) %>%
      ungroup() %>%
      mutate(topic = as.factor(topic),
             term = reorder_within(term, beta, topic)) %>%
      ggplot(aes(x = term, y = beta, fill = topic)) +
      geom_col(show.legend = FALSE) +
      scale_y_continuous(limits = c(0, max_beta)) +
      facet_wrap(~ topic, scales = "free_y") +
      coord_flip() +
      scale_x_reordered() +
      labs(x = NULL, y = "Beta") +
      theme_pubr() +
      theme(axis.text.y = element_text(size = 7))
      
    ggsave(paste0(output_dir, "topwords_facet.png"), 
           plot = p2, width = 12, height = 10, bg="white")
    
    # 8. Plot: Topic Correlations
    png(paste0(output_dir, "corr_network.png"), width = 800, height = 600)
    topic_corr <- topicCorr(topic_model, cutoff = 0.03)
    plot(topic_corr)
    dev.off()
    
    # 10. Plot: Prevalence (Date)
    # This loops through every topic and plots the effect of date
    png(paste0(output_dir, "prevalence.png"), width = 3000, height = 3000, res = 300)
    
    # Set up grid layout
    par(mfrow = c(ceiling(topic_number / 4), 4), mar = c(4, 4, 2, 1))
    
    # Generate axis labels based on date range in meta
    axis_dates <- seq(from = min(as.Date(meta$datetime)), 
                      to = max(as.Date(meta$datetime)), 
                      length.out = 5)
    
    for (k in 1:topic_number) {
      plot.estimateEffect(
        estimate_model,
        covariate = "date_numeric",
        method = "continuous",
        topics = c(k),
        main = paste("Topic", k),
        xlab = "Time",
        linecol = "blue"
      )
    }
    dev.off()
  }
}

# ==============================================================================
# 2. RUN ANALYSIS (Choose Option A or B)
# ==============================================================================

# --- OPTION A: Analyze the EXISTING single model (K=10) ---
# Use this if you just want to analyze the model you ran in the cleaning script.

# Load the specific model
topic_model <- readRDS("models/R_topic_model/stm_topic_model_15.rds")

estimated_model <- readRDS("models/R_topic_model/stm_topic_model_estimated_15.rds")

# Create lists (The function expects lists)
my_models <- list(topic_model = topic_model)
my_estimates <- list(topic_model = estimated_model)

# Run the big analysis function
analyze_topic_model(my_models, my_estimates)
```
