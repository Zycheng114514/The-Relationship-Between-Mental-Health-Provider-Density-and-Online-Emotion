---
title: "final_project"
author: "chris"
format: html
editor: visual
---

```{r}
if (!require("pacman")) install.packages("pacman")

pacman::p_load(
  tidyverse,
  stm,
  lubridate,
  quanteda,
  reshape2,  # For melt()
  tidytext,  # For tidy()
  ggpubr,    # For theme_pubr()
  knitr,     # For kable()
  igraph
)

# ==============================================================================
# 1. LOAD DATA & FUNCTIONS
# ==============================================================================

# Load the data
out <- readRDS("models/inputs/depression/out.rds")
dta <- readRDS("models/inputs/depression/dta.rds")

# Extract components
docs <- out$documents
vocab <- out$vocab
meta <- out$meta

# --- Define Helper Function: Comprehensive Analysis ---
analyze_topic_model <- function(topic_models_list, estimates_list, custom_labels = NULL) {
  
  for (i in seq_along(topic_models_list)) {
    topic_model <- topic_models_list[[i]]
    
    # Check if estimate exists for this model
    topic_key <- names(topic_models_list)[i]
    if (!topic_key %in% names(estimates_list)) {
      warning(paste("No estimate found for", topic_key, "- skipping."))
      next
    }
    estimate_model <- estimates_list[[topic_key]]
    
    topic_number <- topic_model$settings$dim$K
    print(paste("Analyzing Model with K =", topic_number))
    
    # --- Handle Labels ---
    if (is.null(custom_labels) || length(custom_labels) != topic_number) {
      if(!is.null(custom_labels)) warning("Label count does not match K. Using defaults.")
      model_labels <- paste("Topic", 1:topic_number)
    } else {
      model_labels <- custom_labels
    }
    
    # Create a lookup dataframe for easy merging
    label_df <- data.frame(topic = 1:topic_number, label_text = model_labels)
    
    # --- Create Directory ---
    output_dir <- paste0("plots/stm/model_K", topic_number, "/")
    dir.create(output_dir, recursive = TRUE, showWarnings = FALSE)
    
    # 1. Label Topics (Algo based)
    top_words <- labelTopics(topic_model, topics = c(1:topic_number), n = 10, frexweight = 0.5)
    
    # 2. Topic Proportions (Gamma)
    gamma <- tidy(topic_model, matrix = "gamma", document_names = rownames(meta)) %>%
      group_by(topic) %>%
      summarise(gamma = mean(gamma)) %>%
      left_join(label_df, by = "topic") %>%  
      arrange(desc(gamma)) %>%
      mutate(topic_label = paste0(topic, ": ", label_text),
             topic_label = reorder(topic_label, gamma))
    
    # 3. Find Thoughts
    thought <- findThoughts(topic_model, texts = out$meta$text, n = 15)
    
    # 4. Dominant Topic
    dominant_topic <- data.frame(dominant_topic = apply(topic_model$theta, 1, which.max))
    
    # 5. Output Text Report
    sink(paste0(output_dir, "analysis.txt"), split = TRUE)
    print("--- Summary ---")
    summary(topic_model, frexweight = 0.5)
    print("--- Gamma (Expected Proportions) ---")
    print(kable(gamma, digits = 3, col.names = c("Topic", "Mean Gamma", "Label", "Combined")))
    print("--- Top Thoughts ---")
    print(thought)
    sink()
    
    # 6. Plot: Top Word Proportions (Single Plot)
    top_words_1 <- tidy(topic_model, matrix = "beta") %>%
      group_by(topic) %>%
      slice_max(beta, n = 1) %>%
      left_join(label_df, by="topic") %>% 
      arrange(topic, -beta)
    
    p1 <- ggplot(top_words_1, aes(x = beta, y = factor(label_text, levels = model_labels))) +
      geom_col(fill = "gray70") +
      geom_text(aes(label = term), hjust = 0, size = 4) +
      theme_minimal() +
      xlim(0, max(top_words_1$beta) + 0.05) +
      labs(title = paste0("Top Words (K=", topic_number, ")"), x = "Beta", y = "Topic")
    
    ggsave(paste0(output_dir, "top_word_single.png"), plot = p1, bg="white")
    
    # 7. Plot: Top Words Bar Charts (Facet)
    max_beta <- tidy(topic_model, matrix = "beta") %>%
      filter(beta > .001) %>%
      summarise(max = max(beta)) %>% pull(max)
    
    p2 <- tidy(topic_model, matrix = "beta") %>%
      group_by(topic) %>%
      slice_max(beta, n = 10) %>%
      ungroup() %>%
      left_join(label_df, by="topic") %>% 
      mutate(topic_name = factor(label_text, levels = model_labels),
             term = reorder_within(term, beta, topic_name)) %>%
      ggplot(aes(x = term, y = beta, fill = topic_name)) +
      geom_col(show.legend = FALSE) +
      scale_y_continuous(limits = c(0, max_beta)) +
      facet_wrap(~ topic_name, scales = "free_y") + 
      coord_flip() +
      scale_x_reordered() +
      labs(x = NULL, y = "Beta") +
      theme_pubr() +
      theme(axis.text.y = element_text(size = 7))
    
    ggsave(paste0(output_dir, "topwords_facet.png"), 
           plot = p2, width = 14, height = 12, bg="white")
    
    # 8. Plot: Topic Correlations (Network)
    # FIX: Changed 'vertex.label' to 'vlabels'
    png(paste0(output_dir, "corr_network.png"), width = 1000, height = 800)
    topic_corr <- topicCorr(topic_model, cutoff = 0.03)
    tryCatch({
      plot(topic_corr, vlabels = model_labels, vertex.label.cex = 0.8) 
    }, error = function(e) {
      message("Network plot failed (likely low correlation or cutoff issues): ", e$message)
    })
    dev.off()
    
    # 9. Plot: Prevalence (Date)
    png(paste0(output_dir, "prevalence.png"), width = 3000, height = 3000, res = 300)
    
    par(mfrow = c(ceiling(topic_number / 4), 4), mar = c(4, 4, 2, 1))
    
    for (k in 1:topic_number) {
      plot.estimateEffect(
        estimate_model,
        covariate = "date_numeric",
        method = "continuous",
        topics = c(k),
        main = model_labels[k], 
        xlab = "Time",
        linecol = "blue"
      )
    }
    dev.off()
  }
}

# ==============================================================================
# 2. RUN ANALYSIS
# ==============================================================================

# Load your specific model
topic_model <- readRDS("models/R_topic_model/stm_topic_model_15.rds")
estimated_model <- readRDS("models/R_topic_model/stm_topic_model_estimated_15.rds")

# Prepare lists
my_models <- list(topic_model = topic_model)
my_estimates <- list(topic_model = estimated_model)

# ------------------------------------------------------------------------------
# INPUT REQUIRED: DEFINE YOUR LABELS HERE
# ------------------------------------------------------------------------------
# Since you have K=15, you must provide exactly 15 labels in order (1 to 15).
# Replace these strings with your actual topic names based on your previous analysis.

my_topic_labels <- c(
  "1. Sports Venting",             # Aggressive sports ranting (Bruins/Columbus)
  "2. Chicago 'Depression' Dogs",  # Specific hot dog style & pets
  "3. NBA/Celtics Fandom",         # Basketball performance discussions
  "4. Clinical Treatment/Meds",    # Therapy, psychiatrists, prescriptions
  "5. Politics & Economy",         # Elections, government, tariffs
  "6. Suicide Resources",          # Hotlines and help bots
  "7. Transgender Mental Health",  # Gender-affirming care studies
  "8. Casual Reactions",           # Calling things "depressing"
  "9. Suicide Philosophy (DFW)",   # David Foster Wallace quotes
  "10. Urban Planning & Transit",  # Public transport and city infrastructure
  "11. Loneliness & Socializing",  # Making friends and advice
  "12. Seasonal Affective Disorder", # Weather, winter, Seattle/Chicago
  "13. Anecdotes & Lyrics",        # Personal stories and song lyrics
  "14. Cost of Living/Housing",    # Rent, budgets, real estate
  "15. Academic Struggles"         # GPA, failing classes, university stress
)

# ------------------------------------------------------------------------------
# Run the Function with Labels
# ------------------------------------------------------------------------------

analyze_topic_model(my_models, my_estimates, custom_labels = my_topic_labels)
```
